--- ./parser_test.go.orig	2025-02-27 18:57:18.000000000 +0100
+++ ./parser_test.go	2025-04-04 16:56:26.893372794 +0200
@@ -81,14 +81,6 @@
 	}
 }
 
-func TestIssue42951(t *testing.T) {
-	path := "./testdata/issue42951"
-	_, err := ParseDir(token.NewFileSet(), path, nil, 0)
-	if err != nil {
-		t.Errorf("ParseDir(%s): %v", path, err)
-	}
-}
-
 func TestParseExpr(t *testing.T) {
 	// just kicking the tires:
 	// a valid arithmetic expression
@@ -391,6 +383,7 @@
 }
 
 func checkFieldComments(t *testing.T, file *ast.File, fieldname, lead, line string) {
+	t.Helper()
 	f := getField(file, fieldname)
 	if f == nil {
 		t.Fatalf("field not found: %s", fieldname)
@@ -680,7 +673,7 @@
 						t.Errorf("ParseFile(...): %v (want success)", err)
 					}
 				} else {
-					expected := "exceeded max nesting depth"
+					expected := "exceeded max nesting depth 100000"
 					if err == nil || !strings.HasSuffix(err.Error(), expected) {
 						t.Errorf("ParseFile(...) = _, %v, want %q", err, expected)
 					}
--- ./interface.go.orig	2025-02-27 18:57:18.000000000 +0100
+++ ./interface.go	2025-04-04 16:56:26.893783754 +0200
@@ -120,12 +120,63 @@
 	}()
 
 	// parse source
-	p.init(fset, filename, text, mode)
+	p.init(fset, filename, text, mode, 0)
 	f = p.parseFile()
 
 	return
 }
 
+type Stats struct {
+	TopNest int
+	NumTok  int
+}
+
+func ParseFileStats(fset *token.FileSet, filename string, src any, mode Mode, maxNest int) (f *ast.File, s *Stats, err error) {
+	if fset == nil {
+		panic("parser.ParseFile: no token.FileSet provided (fset == nil)")
+	}
+
+	// get source
+	text, err := readSource(filename, src)
+	if err != nil {
+		return nil, nil, err
+	}
+
+	var p parser
+	defer func() {
+		if e := recover(); e != nil {
+			// resume same panic if it's not a bailout
+			bail, ok := e.(bailout)
+			if !ok {
+				panic(e)
+			} else if bail.msg != "" {
+				p.errors.Add(p.file.Position(bail.pos), bail.msg)
+			}
+		}
+
+		// set result values
+		if f == nil {
+			// source is not a valid Go source file - satisfy
+			// ParseFile API and return a valid (but) empty
+			// *ast.File
+			f = &ast.File{
+				Name:  new(ast.Ident),
+				Scope: ast.NewScope(nil),
+			}
+		}
+
+		p.errors.Sort()
+		err = p.errors.Err()
+	}()
+
+	// parse source
+	p.init(fset, filename, text, mode, maxNest)
+	f = p.parseFile()
+	s = &Stats{p.topNest, p.numTok}
+
+	return
+}
+
 // ParseDir calls [ParseFile] for all files with names ending in ".go" in the
 // directory specified by path and returns a map of package name -> package
 // AST with all the packages found.
@@ -215,7 +266,47 @@
 	}()
 
 	// parse expr
-	p.init(fset, filename, text, mode)
+	p.init(fset, filename, text, mode, 0)
+	expr = p.parseRhs()
+
+	// If a semicolon was inserted, consume it;
+	// report an error if there's more tokens.
+	if p.tok == token.SEMICOLON && p.lit == "\n" {
+		p.next()
+	}
+	p.expect(token.EOF)
+
+	return
+}
+
+func ParseExprFromStats(fset *token.FileSet, filename string, src any, mode Mode, maxNest int) (expr ast.Expr, s *Stats, err error) {
+	if fset == nil {
+		panic("parser.ParseExprFrom: no token.FileSet provided (fset == nil)")
+	}
+
+	// get source
+	text, err := readSource(filename, src)
+	if err != nil {
+		return nil, nil, err
+	}
+
+	var p parser
+	defer func() {
+		if e := recover(); e != nil {
+			// resume same panic if it's not a bailout
+			bail, ok := e.(bailout)
+			if !ok {
+				panic(e)
+			} else if bail.msg != "" {
+				p.errors.Add(p.file.Position(bail.pos), bail.msg)
+			}
+		}
+		p.errors.Sort()
+		err = p.errors.Err()
+	}()
+
+	// parse expr
+	p.init(fset, filename, text, mode, 0)
 	expr = p.parseRhs()
 
 	// If a semicolon was inserted, consume it;
@@ -238,3 +329,7 @@
 func ParseExpr(x string) (ast.Expr, error) {
 	return ParseExprFrom(token.NewFileSet(), "", []byte(x), 0)
 }
+
+func ParseExprStats(x string, maxNest int) (ast.Expr, *Stats, error) {
+	return ParseExprFromStats(token.NewFileSet(), "", []byte(x), 0, maxNest)
+}
--- ./performance_test.go.orig	2025-02-27 18:57:18.000000000 +0100
+++ ./performance_test.go	2025-04-04 16:56:26.894037839 +0200
@@ -10,7 +10,7 @@
 	"testing"
 )
 
-var src = readFile("../printer/nodes.go")
+var src = readFile("nodes.go.src")
 
 func readFile(filename string) []byte {
 	data, err := os.ReadFile(filename)
--- ./resolver.go.orig	2025-02-27 18:57:18.000000000 +0100
+++ ./resolver.go	2025-04-04 16:56:26.894467633 +0200
@@ -253,7 +253,6 @@
 	}
 
 	switch n := node.(type) {
-
 	// Expressions.
 	case *ast.Ident:
 		r.resolve(n, true)
--- ./parser.go.orig	2025-02-27 18:57:18.000000000 +0100
+++ ./parser.go	2025-04-04 16:56:26.895620638 +0200
@@ -19,10 +19,11 @@
 	"fmt"
 	"go/ast"
 	"go/build/constraint"
-	"go/internal/typeparams"
 	"go/scanner"
 	"go/token"
 	"strings"
+
+	"github.com/gnolang/gno/gnovm/pkg/parser/internal/typeparams"
 )
 
 // The parser structure holds the parser's internal state.
@@ -64,13 +65,26 @@
 	// nestLev is used to track and limit the recursion depth
 	// during parsing.
 	nestLev int
+
+	// maxNest is the maximum nesting level authorized for this parser.
+	maxNest int
+
+	// topNest is the maximum nesting level reached durin parsing.
+	topNest int
+
+	// numTok is the number of parsed tokens.
+	numTok int
 }
 
-func (p *parser) init(fset *token.FileSet, filename string, src []byte, mode Mode) {
+func (p *parser) init(fset *token.FileSet, filename string, src []byte, mode Mode, maxNest int) {
 	p.file = fset.AddFile(filename, -1, len(src))
 	eh := func(pos token.Position, msg string) { p.errors.Add(pos, msg) }
 	p.scanner.Init(p.file, src, eh, scanner.ScanComments)
 
+	p.maxNest = maxNest
+	if p.maxNest == 0 {
+		p.maxNest = maxNestLev
+	}
 	p.top = true
 	p.mode = mode
 	p.trace = mode&Trace != 0 // for convenience (p.trace is used frequently)
@@ -112,10 +126,13 @@
 
 func incNestLev(p *parser) *parser {
 	p.nestLev++
-	if p.nestLev > maxNestLev {
-		p.error(p.pos, "exceeded max nesting depth")
+	if p.nestLev > p.maxNest {
+		p.error(p.pos, fmt.Sprintf("exceeded max nesting depth %d", p.maxNest))
 		panic(bailout{})
 	}
+	if p.topNest < p.nestLev {
+		p.topNest = p.nestLev
+	}
 	return p
 }
 
@@ -145,6 +162,7 @@
 
 	for {
 		p.pos, p.tok, p.lit = p.scanner.Scan()
+		p.numTok++
 		if p.tok == token.COMMENT {
 			if p.top && strings.HasPrefix(p.lit, "//go:build") {
 				if x, err := constraint.Parse(p.lit); err == nil {
@@ -571,19 +589,19 @@
 
 // "[" has already been consumed, and lbrack is its position.
 // If len != nil it is the already consumed array length.
-func (p *parser) parseArrayType(lbrack token.Pos, len ast.Expr) *ast.ArrayType {
+func (p *parser) parseArrayType(lbrack token.Pos, length ast.Expr) *ast.ArrayType {
 	if p.trace {
 		defer un(trace(p, "ArrayType"))
 	}
 
-	if len == nil {
+	if length == nil {
 		p.exprLev++
 		// always permit ellipsis for more fault-tolerant parsing
 		if p.tok == token.ELLIPSIS {
-			len = &ast.Ellipsis{Ellipsis: p.pos}
+			length = &ast.Ellipsis{Ellipsis: p.pos}
 			p.next()
 		} else if p.tok != token.RBRACK {
-			len = p.parseRhs()
+			length = p.parseRhs()
 		}
 		p.exprLev--
 	}
@@ -596,7 +614,7 @@
 	}
 	p.expect(token.RBRACK)
 	elt := p.parseType()
-	return &ast.ArrayType{Lbrack: lbrack, Len: len, Elt: elt}
+	return &ast.ArrayType{Lbrack: lbrack, Len: length, Elt: elt}
 }
 
 func (p *parser) parseArrayFieldOrTypeInstance(x *ast.Ident) (*ast.Ident, ast.Expr) {
@@ -1016,7 +1034,7 @@
 
 	// If the parameter list consists of named parameters with types,
 	// collect all names with the same types into a single ast.Field.
-	var names []*ast.Ident
+	names := []*ast.Ident{}
 	var typ ast.Expr
 	addParams := func() {
 		assert(typ != nil, "nil type in named parameter list")
@@ -2481,7 +2499,7 @@
 // ----------------------------------------------------------------------------
 // Declarations
 
-type parseSpecFunction func(doc *ast.CommentGroup, keyword token.Token, iota int) ast.Spec
+type parseSpecFunction func(doc *ast.CommentGroup, keyword token.Token, i int) ast.Spec
 
 func (p *parser) parseImportSpec(doc *ast.CommentGroup, _ token.Token, _ int) ast.Spec {
 	if p.trace {
@@ -2523,7 +2541,7 @@
 	return spec
 }
 
-func (p *parser) parseValueSpec(doc *ast.CommentGroup, keyword token.Token, iota int) ast.Spec {
+func (p *parser) parseValueSpec(doc *ast.CommentGroup, keyword token.Token, i int) ast.Spec {
 	if p.trace {
 		defer un(trace(p, keyword.String()+"Spec"))
 	}
@@ -2733,8 +2751,8 @@
 	if p.tok == token.LPAREN {
 		lparen = p.pos
 		p.next()
-		for iota := 0; p.tok != token.RPAREN && p.tok != token.EOF; iota++ {
-			list = append(list, f(p.leadComment, keyword, iota))
+		for i := 0; p.tok != token.RPAREN && p.tok != token.EOF; i++ {
+			list = append(list, f(p.leadComment, keyword, i))
 		}
 		rparen = p.expect(token.RPAREN)
 		p.expectSemi()
